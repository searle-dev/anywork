# ╔══════════════════════════════════════════════════════════════╗
# ║  AnyWork Configuration                                     ║
# ║  Copy to .env and fill in your values                      ║
# ╚══════════════════════════════════════════════════════════════╝

# === LLM Provider ===
# AnyWork supports ANY OpenAI-compatible API (default) or Anthropic native.
# Just set API_BASE_URL + API_KEY + MODEL — that's it.

# --- Option 1: OpenAI-compatible (recommended, covers 95%+ providers) ---
API_STYLE=openai
API_BASE_URL=https://openrouter.ai/api/v1
API_KEY=sk-or-xxxxx
MODEL=anthropic/claude-sonnet-4-20250514

# --- Option 2: Anthropic native ---
# API_STYLE=anthropic
# API_KEY=sk-ant-xxxxx
# MODEL=claude-sonnet-4-20250514

# --- Common provider presets (just change API_BASE_URL and API_KEY) ---
# OpenRouter:   API_BASE_URL=https://openrouter.ai/api/v1
# KIMI:         API_BASE_URL=https://api.moonshot.cn/v1       MODEL=moonshot-v1-8k
# DeepSeek:     API_BASE_URL=https://api.deepseek.com/v1      MODEL=deepseek-chat
# SiliconFlow:  API_BASE_URL=https://api.siliconflow.cn/v1    MODEL=deepseek-ai/DeepSeek-V3
# Groq:         API_BASE_URL=https://api.groq.com/openai/v1   MODEL=llama-3.3-70b-versatile
# Together:     API_BASE_URL=https://api.together.xyz/v1      MODEL=meta-llama/Llama-3-70b-chat-hf
# Ollama:       API_BASE_URL=http://host.docker.internal:11434/v1  MODEL=llama3.2
# vLLM:         API_BASE_URL=http://host.docker.internal:8000/v1   MODEL=your-model
# OpenAI:       API_BASE_URL=https://api.openai.com/v1        MODEL=gpt-4o
# Azure OpenAI: API_BASE_URL=https://{resource}.openai.azure.com/openai/deployments/{deployment}

# Legacy (backward compatible, auto-detected):
# ANTHROPIC_API_KEY=sk-ant-xxxxx
# OPENAI_API_KEY=sk-xxxxx

# Web Search (optional) — get key at https://brave.com/search/api/
# Leave empty to disable the web_search tool
BRAVE_API_KEY=

# === Server ===
SERVER_PORT=3001
JWT_SECRET=change-me-in-production

# === Worker ===
WORKER_PORT=8080
WORKER_IMAGE=anywork-worker:latest

# === Storage ===
# local | filestore
STORAGE_DRIVER=local
LOCAL_DATA_DIR=/data/users

# === Container Scheduler ===
# static (docker-compose) | docker (per-user) | k8s (multi-user, recommended) | cloudrun (Phase 2)
CONTAINER_DRIVER=static

# === Kubernetes Driver (CONTAINER_DRIVER=k8s) ===
# Namespace where worker Pods and Services are created
K8S_NAMESPACE=anywork

# Workspace storage mode:
#   emptydir  – ephemeral per-session workspace (simplest, no PVC needed)
#   pvc       – persistent per-user PVC (files survive session restarts)
K8S_WORKSPACE_STORAGE=emptydir

# StorageClass for PVCs (only used when K8S_WORKSPACE_STORAGE=pvc)
# K8S_PVC_STORAGE_CLASS=standard

# Worker Pod resource limits
K8S_CPU_REQUEST=250m
K8S_CPU_LIMIT=2000m
K8S_MEMORY_REQUEST=512Mi
K8S_MEMORY_LIMIT=2Gi

# Seconds before an idle worker Pod is garbage-collected (0 = disabled)
K8S_IDLE_TTL_SECONDS=1800

# === Database ===
DATABASE_URL=sqlite:///data/anywork.db

# === Google Cloud (Phase 2) ===
# GCP_PROJECT_ID=
# GCP_REGION=us-central1
# FILESTORE_IP=
# FILESTORE_SHARE=anywork_share
